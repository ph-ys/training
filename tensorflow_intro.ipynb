{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "- TensorFlow とは機械学習のアルゴリズムを実装して実行するためのプログラミングインターフェースである。\n",
    "- TensorFlow は一連のノードからなる計算グラフに基づいている。各ノードは0個以上の入力または出力をもつ演算を表す。これらの演算の入力と出力を参照するシンボリックハンドルとしてテンソルが作成される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テンソルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 21:11:40.602692: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-16 21:11:40.812191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-16 21:11:40.902933: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-16 21:11:40.935029: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-16 21:11:41.095592: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-16 21:11:42.061431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np array を作成する\n",
    "a = np.array([1, 2, 3], dtype=np.int32)\n",
    "# 普通に list を作成する\n",
    "b = [4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor([4 5 6], shape=(3,), dtype=int32)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# どちらからもコンバートすることができる\n",
    "t_a = tf.convert_to_tensor(a)\n",
    "t_b = tf.convert_to_tensor(b)\n",
    "print(t_a)\n",
    "print(t_b)\n",
    "print(t_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_ones = tf.ones((2, 3))\n",
    "t_ones.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1.2   5.    3.142], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# constant で直接作成することもできる（convert_to_tensorとどう違うのか）\n",
    "c_tensor = tf.constant([1.2, 5, np.pi], dtype=tf.float32)\n",
    "print(c_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Dataset API\n",
    "効率的で便利な前処理パイプラインを構築するためのクラスがあり、以下の必要性にこたえている。\n",
    "- メモリに乗らない大きいデータセットをバッチ（ミニバッチ）ごとに読み込む必要性\n",
    "- データに特定の変換や前処理を適用する必要性\n",
    "\n",
    "バッチごとに学習できるのはDNNが反復的な最適化アルゴリズム（確率的勾配降下法など）を使ってモデルを増分的に訓練しているからである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの作成は大きく3つある。\n",
    "- 既存のテンソルから作成\n",
    "- ローカルのファイルから作成\n",
    "- tensorflow_datasets ライブラリから取り出す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 既存のテンソルから作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.float32, name=None)>\n"
     ]
    }
   ],
   "source": [
    "a = [1.2, 3.4, 7.5, 4.1, 5.0, 1.0]\n",
    "ds = tf.data.Dataset.from_tensor_slices(a)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1, [1.2 3.4 7.5]\n",
      "batch 2, [4.1 5.  1. ]\n"
     ]
    }
   ],
   "source": [
    "ds_batch = ds.batch(3) # 3要素ずつのミニバッチを作成する\n",
    "for i, elem in enumerate(ds_batch, 1):\n",
    "    print(f'batch {i}, {elem.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ローカルファイルから作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 省略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow_datasets ライブラリから取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 21:21:30.120685: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291\n"
     ]
    }
   ],
   "source": [
    "print(len(tfds.list_builders()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeaturesDict({\n",
      "    'attributes': FeaturesDict({\n",
      "        '5_o_Clock_Shadow': bool,\n",
      "        'Arched_Eyebrows': bool,\n",
      "        'Attractive': bool,\n",
      "        'Bags_Under_Eyes': bool,\n",
      "        'Bald': bool,\n",
      "        'Bangs': bool,\n",
      "        'Big_Lips': bool,\n",
      "        'Big_Nose': bool,\n",
      "        'Black_Hair': bool,\n",
      "        'Blond_Hair': bool,\n",
      "        'Blurry': bool,\n",
      "        'Brown_Hair': bool,\n",
      "        'Bushy_Eyebrows': bool,\n",
      "        'Chubby': bool,\n",
      "        'Double_Chin': bool,\n",
      "        'Eyeglasses': bool,\n",
      "        'Goatee': bool,\n",
      "        'Gray_Hair': bool,\n",
      "        'Heavy_Makeup': bool,\n",
      "        'High_Cheekbones': bool,\n",
      "        'Male': bool,\n",
      "        'Mouth_Slightly_Open': bool,\n",
      "        'Mustache': bool,\n",
      "        'Narrow_Eyes': bool,\n",
      "        'No_Beard': bool,\n",
      "        'Oval_Face': bool,\n",
      "        'Pale_Skin': bool,\n",
      "        'Pointy_Nose': bool,\n",
      "        'Receding_Hairline': bool,\n",
      "        'Rosy_Cheeks': bool,\n",
      "        'Sideburns': bool,\n",
      "        'Smiling': bool,\n",
      "        'Straight_Hair': bool,\n",
      "        'Wavy_Hair': bool,\n",
      "        'Wearing_Earrings': bool,\n",
      "        'Wearing_Hat': bool,\n",
      "        'Wearing_Lipstick': bool,\n",
      "        'Wearing_Necklace': bool,\n",
      "        'Wearing_Necktie': bool,\n",
      "        'Young': bool,\n",
      "    }),\n",
      "    'identity': FeaturesDict({\n",
      "        'Identity_No': int64,\n",
      "    }),\n",
      "    'image': Image(shape=(218, 178, 3), dtype=uint8),\n",
      "    'landmarks': FeaturesDict({\n",
      "        'lefteye_x': int64,\n",
      "        'lefteye_y': int64,\n",
      "        'leftmouth_x': int64,\n",
      "        'leftmouth_y': int64,\n",
      "        'nose_x': int64,\n",
      "        'nose_y': int64,\n",
      "        'righteye_x': int64,\n",
      "        'righteye_y': int64,\n",
      "        'rightmouth_x': int64,\n",
      "        'rightmouth_y': int64,\n",
      "    }),\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "celeba_bldr = tfds.builder('celeb_a')\n",
    "print(celeba_bldr.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image(shape=(218, 178, 3), dtype=uint8)\n",
      "dict_keys(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'])\n",
      "@inproceedings{conf/iccv/LiuLWT15,\n",
      "  added-at = {2018-10-09T00:00:00.000+0200},\n",
      "  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n",
      "  biburl = {https://www.bibsonomy.org/bibtex/250e4959be61db325d2f02c1d8cd7bfbb/dblp},\n",
      "  booktitle = {ICCV},\n",
      "  crossref = {conf/iccv/2015},\n",
      "  ee = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.425},\n",
      "  interhash = {3f735aaa11957e73914bbe2ca9d5e702},\n",
      "  intrahash = {50e4959be61db325d2f02c1d8cd7bfbb},\n",
      "  isbn = {978-1-4673-8391-2},\n",
      "  keywords = {dblp},\n",
      "  pages = {3730-3738},\n",
      "  publisher = {IEEE Computer Society},\n",
      "  timestamp = {2018-10-11T11:43:28.000+0200},\n",
      "  title = {Deep Learning Face Attributes in the Wild.},\n",
      "  url = {http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15},\n",
      "  year = 2015\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(celeba_bldr.info.features['image'])\n",
    "print(celeba_bldr.info.features['attributes'].keys())\n",
    "print(celeba_bldr.info.citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 1.39 GiB (download: 1.39 GiB, generated: 1.63 GiB, total: 3.01 GiB) to /home/hoyo/tensorflow_datasets/celeb_a/2.1.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|██████████| 45137925/45137925 [00:00<00:00, 69700965.01 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 5/5 [00:00<00:00,  7.68 url/s]\n"
     ]
    },
    {
     "ename": "NonMatchingChecksumError",
     "evalue": "Artifact https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pZjFTYXZWM3FlRnM, downloaded to /home/hoyo/tensorflow_datasets/downloads/ucexport_download_id_0B7EVK8r0v71pZjFTYXZWM3FlDDaXUAQO8EGH_a7VqGNLRtW52mva1LzDrb-V723OQN8.tmp.370418b583e4436f9185e8a11d8a3b87/download, has wrong checksum:\n* Expected: UrlInfo(size=1.34 GiB, checksum='46fb89443c578308acf364d7d379fe1b9efb793042c0af734b6112e4fd3a8c74', filename='img_align_celeba.zip')\n* Got: UrlInfo(size=2.37 KiB, checksum='1b86097ad19a8cb93cdd7558afae77a7c3db6b7cc9e88e9acd0965d9c0594590', filename='download')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNonMatchingChecksumError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mceleba_bldr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m datasets \u001b[38;5;241m=\u001b[39m celeba_bldr\u001b[38;5;241m.\u001b[39mas_dataset(shuffle_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m datasets\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:699\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    697\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[1;32m    705\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[1;32m    706\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[1;32m    707\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[1;32m    708\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:1669\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download_config\u001b[38;5;241m.\u001b[39mmax_examples_per_split \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1667\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1669\u001b[0m split_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m splits_lib\u001b[38;5;241m.\u001b[39mSplitDict(split_infos)\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:1620\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._generate_splits\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m   optional_pipeline_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1620\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[1;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_pipeline_kwargs\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[1;32m   1626\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[1;32m   1627\u001b[0m     split_generators\u001b[38;5;241m=\u001b[39msplit_generators,\n\u001b[1;32m   1628\u001b[0m     generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples,\n\u001b[1;32m   1629\u001b[0m     is_beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[1;32m   1630\u001b[0m )\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tensorflow_datasets/datasets/celeb_a/celeb_a_dataset_builder.py:100\u001b[0m, in \u001b[0;36mBuilder._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split_generators\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[0;32m--> 100\u001b[0m   downloaded_dirs \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg_align_celeba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_ALIGNED_DATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlist_eval_partition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mEVAL_LIST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlist_attr_celeba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mATTR_DATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlandmarks_celeba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mLANDMARKS_DATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43midentity_celeba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mIDENTITY_DATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# Load all images in memory (~1 GiB)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m   \u001b[38;5;66;03m# Use split to convert: `img_align_celeba/000005.jpg` -> `000005.jpg`\u001b[39;00m\n\u001b[1;32m    110\u001b[0m   all_images \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m       os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(k)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]: img\n\u001b[1;32m    112\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m k, img \u001b[38;5;129;01min\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39miter_archive(\n\u001b[1;32m    113\u001b[0m           downloaded_dirs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_align_celeba\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    114\u001b[0m       )\n\u001b[1;32m    115\u001b[0m   }\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:607\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# Add progress bar to follow the download state\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[0;32m--> 607\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:791\u001b[0m, in \u001b[0;36m_map_promise\u001b[0;34m(map_fn, all_inputs)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    790\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tree/__init__.py:428\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    426\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 428\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:791\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    790\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m res \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises)  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/promise/promise.py:512\u001b[0m, in \u001b[0;36mPromise.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    510\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target()\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(timeout \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TIMEOUT)\n\u001b[0;32m--> 512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/promise/promise.py:516\u001b[0m, in \u001b[0;36mPromise._target_settled_value\u001b[0;34m(self, _raise)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_target_settled_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, _raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# type: (bool) -> Any\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/promise/promise.py:226\u001b[0m, in \u001b[0;36mPromise._settled_value\u001b[0;34m(self, _raise)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _raise:\n\u001b[1;32m    225\u001b[0m     raise_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n\u001b[0;32m--> 226\u001b[0m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_traceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/promise/promise.py:87\u001b[0m, in \u001b[0;36mtry_catch\u001b[0;34m(handler, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_catch\u001b[39m(handler, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# type: (Callable, Any, Any) -> Union[Tuple[Any, None], Tuple[None, Tuple[Exception, Optional[TracebackType]]]]\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     89\u001b[0m         tb \u001b[38;5;241m=\u001b[39m exc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:414\u001b[0m, in \u001b[0;36mDownloadManager._download.<locals>.<lambda>\u001b[0;34m(dl_result)\u001b[0m\n\u001b[1;32m    408\u001b[0m   future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[1;32m    409\u001b[0m       url, download_tmp_dir, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_ssl\n\u001b[1;32m    410\u001b[0m   )\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# Post-process the result\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mthen(\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m dl_result: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_or_validate_checksums\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=g-long-lambda\u001b[39;49;00m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomputed_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m )\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:471\u001b[0m, in \u001b[0;36mDownloadManager._register_or_validate_checksums\u001b[0;34m(self, path, url, expected_url_info, computed_url_info, checksum_path, url_path)\u001b[0m\n\u001b[1;32m    460\u001b[0m   checksum_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dl_path(url, computed_url_info\u001b[38;5;241m.\u001b[39mchecksum)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m   \u001b[38;5;66;03m# Eventually validate checksums\u001b[39;00m\n\u001b[1;32m    463\u001b[0m   \u001b[38;5;66;03m# Note:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m   \u001b[38;5;66;03m#   download). This is expected as it might mean the downloaded file\u001b[39;00m\n\u001b[1;32m    470\u001b[0m   \u001b[38;5;66;03m#   was corrupted. Note: The tmp file isn't deleted to allow inspection.\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m   \u001b[43m_validate_checksums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m      \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexpected_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomputed_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputed_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m      \u001b[49m\u001b[43mforce_checksums_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_checksums_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rename_and_get_final_dl_path(\n\u001b[1;32m    480\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    481\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m     url_path\u001b[38;5;241m=\u001b[39murl_path,\n\u001b[1;32m    486\u001b[0m )\n",
      "File \u001b[0;32m~/work/training/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:771\u001b[0m, in \u001b[0;36m_validate_checksums\u001b[0;34m(url, path, computed_url_info, expected_url_info, force_checksums_validation)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    760\u001b[0m     expected_url_info\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m computed_url_info\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m expected_url_info \u001b[38;5;241m!=\u001b[39m computed_url_info\n\u001b[1;32m    763\u001b[0m ):\n\u001b[1;32m    764\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    765\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArtifact \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, downloaded to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, has wrong checksum:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    766\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m* Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_url_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    770\u001b[0m   )\n\u001b[0;32m--> 771\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m NonMatchingChecksumError(msg)\n",
      "\u001b[0;31mNonMatchingChecksumError\u001b[0m: Artifact https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pZjFTYXZWM3FlRnM, downloaded to /home/hoyo/tensorflow_datasets/downloads/ucexport_download_id_0B7EVK8r0v71pZjFTYXZWM3FlDDaXUAQO8EGH_a7VqGNLRtW52mva1LzDrb-V723OQN8.tmp.370418b583e4436f9185e8a11d8a3b87/download, has wrong checksum:\n* Expected: UrlInfo(size=1.34 GiB, checksum='46fb89443c578308acf364d7d379fe1b9efb793042c0af734b6112e4fd3a8c74', filename='img_align_celeba.zip')\n* Got: UrlInfo(size=2.37 KiB, checksum='1b86097ad19a8cb93cdd7558afae77a7c3db6b7cc9e88e9acd0965d9c0594590', filename='download')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror"
     ]
    }
   ],
   "source": [
    "celeba_bldr.download_and_prepare()\n",
    "datasets = celeba_bldr.as_dataset(shuffle_files=False)\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://qiita.com/takkeybook/items/358e57f0706367e83be6 で解決可能だが面倒なので省略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
